{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae96e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "glove.6B.300d.txt already exists. Skipping download/unzip.\n",
      "\n",
      "--- Loading Raw Data ---\n",
      "Successfully read 254090 lines from data/en_sents\n",
      "Successfully read 254090 lines from data/vi_sents\n",
      "\n",
      "--- Splitting Data ---\n",
      "Train size: 203272\n",
      "Validation size: 25409\n",
      "Test size: 25409\n",
      "\n",
      "--- Preparing Tokenizers ---\n",
      "Loading existing tokenizer for en from tokenizers/en_tokenizer.json\n",
      "Enabled padding for en tokenizer (PAD ID: 0).\n",
      "Loading existing tokenizer for vi from tokenizers/vi_tokenizer.json\n",
      "Enabled padding for vi tokenizer (PAD ID: 0).\n",
      "English Vocab Size: 20000\n",
      "Vietnamese Vocab Size: 9660\n",
      "\n",
      "--- Loading GloVe Embeddings ---\n",
      "Loading GloVe embeddings from glove_data/glove.6B.300d.txt...\n",
      "Tokenizer vocab size (for embedding matrix): 20000\n",
      "Loaded 14781/20000 words from GloVe file into embedding matrix.\n",
      "Set PAD token embedding (Index: 0) to zeros.\n",
      "Initializing UNK token embedding (Index: 3) randomly.\n",
      "Initializing SOS token embedding (Index: 1) randomly.\n",
      "Initializing EOS token embedding (Index: 2) randomly.\n",
      "\n",
      "--- Creating Datasets and DataLoaders ---\n",
      "DataLoaders created.\n",
      "\n",
      "--- Initializing Model ---\n",
      "Encoder: Initializing embedding layer with pre-trained weights.\n",
      "Encoder embedding layer created with shape: torch.Size([20000, 300]), Frozen: True\n",
      "Decoder: Initializing embedding layer randomly.\n",
      "Decoder embedding layer created with shape: torch.Size([9660, 256])\n",
      "Skipping initialization for encoder.embedding.weight (pre-trained)\n",
      "Model initialized.\n",
      "Optimizer and Criterion set.\n",
      "\n",
      "--- Starting Training Loop ---\n",
      "\n",
      "Epoch: 01/10\n",
      "Starting training epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-> Saved Best Model (Val Loss: 4.918)\n",
      "\tTime: 91m 13s\n",
      "\tTrain Loss: 4.985 | Train PPL: 146.186\n",
      "\t Val. Loss: 4.918 |  Val. PPL: 136.756\n",
      "\n",
      "Epoch: 02/10\n",
      "Starting training epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-> Saved Best Model (Val Loss: 4.344)\n",
      "\tTime: 136m 17s\n",
      "\tTrain Loss: 3.977 | Train PPL:  53.383\n",
      "\t Val. Loss: 4.344 |  Val. PPL:  76.977\n",
      "\n",
      "Epoch: 03/10\n",
      "Starting training epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-> Saved Best Model (Val Loss: 3.972)\n",
      "\tTime: 54m 56s\n",
      "\tTrain Loss: 3.387 | Train PPL:  29.586\n",
      "\t Val. Loss: 3.972 |  Val. PPL:  53.114\n",
      "\n",
      "Epoch: 04/10\n",
      "Starting training epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-> Saved Best Model (Val Loss: 3.718)\n",
      "\tTime: 32m 58s\n",
      "\tTrain Loss: 3.014 | Train PPL:  20.363\n",
      "\t Val. Loss: 3.718 |  Val. PPL:  41.186\n",
      "\n",
      "Epoch: 05/10\n",
      "Starting training epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-> Saved Best Model (Val Loss: 3.476)\n",
      "\tTime: 30m 47s\n",
      "\tTrain Loss: 2.750 | Train PPL:  15.641\n",
      "\t Val. Loss: 3.476 |  Val. PPL:  32.321\n",
      "\n",
      "Epoch: 06/10\n",
      "Starting training epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-> Saved Best Model (Val Loss: 3.340)\n",
      "\tTime: 30m 3s\n",
      "\tTrain Loss: 2.539 | Train PPL:  12.672\n",
      "\t Val. Loss: 3.340 |  Val. PPL:  28.214\n",
      "\n",
      "Epoch: 07/10\n",
      "Starting training epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████  | 1284/1589 [24:12<12:58,  2.55s/it, loss=2.01]"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Cần dòng trên nếu chạy trên môi trường không mặc định UTF-8\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Hugging Face Tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing # For SOS/EOS tokens\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import requests # For downloading GloVe\n",
    "from tqdm import tqdm # Progress bar for download\n",
    "import zipfile # For unzipping GloVe\n",
    "import traceback # For printing detailed errors\n",
    "\n",
    "# --- Constants ---\n",
    "DATA_DIR = 'data'\n",
    "EN_FILE = os.path.join(DATA_DIR, 'en_sents')\n",
    "VI_FILE = os.path.join(DATA_DIR, 'vi_sents')\n",
    "TOKENIZER_DIR = 'tokenizers' # Thư mục lưu tokenizer đã huấn luyện\n",
    "GLOVE_DIR = 'glove_data'   # Thư mục chứa file GloVe\n",
    "GLOVE_ZIP_URL = 'http://nlp.stanford.edu/data/glove.6B.zip' # Link tải GloVe (ví dụ)\n",
    "GLOVE_ZIP_FILENAME = 'glove.6B.zip' # Tên file zip\n",
    "GLOVE_FILENAME = 'glove.6B.300d.txt' # Chọn file GloVe (ví dụ 300 chiều)\n",
    "GLOVE_PATH = os.path.join(GLOVE_DIR, GLOVE_FILENAME)\n",
    "MODEL_SAVE_PATH = 'seq2seq-gru-glove-hf.pt' # Tên file lưu model\n",
    "\n",
    "# Special tokens (phù hợp với Hugging Face Tokenizers)\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "SOS_TOKEN = \"[SOS]\" # Hoặc [CLS] tùy bạn chọn\n",
    "EOS_TOKEN = \"[EOS]\" # Hoặc [SEP] tùy bạn chọn\n",
    "SPECIAL_TOKENS = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "MAX_VOCAB_SIZE = 20000     # Giới hạn kích thước từ điển (ví dụ)\n",
    "EMBEDDING_DIM_GLOVE = 300 # Phải khớp với file GloVe bạn dùng\n",
    "EMBEDDING_DIM_VI = 256   # Kích thước embedding cho tiếng Việt (có thể khác GloVe)\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT_RATE = 0.3\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128       # Giảm nếu gặp lỗi Out-of-Memory (OOM)\n",
    "NUM_EPOCHS = 10      # Số epoch huấn luyện (ví dụ)\n",
    "CLIP = 1.0                # Giá trị clipping gradient\n",
    "TEACHER_FORCING_RATIO = 0.5 # Tỷ lệ sử dụng teacher forcing khi huấn luyện\n",
    "FREEZE_GLOVE = True       # Có đóng băng embedding GloVe hay không\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def normalize_string(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    # Có thể thêm các bước làm sạch khác nếu cần\n",
    "    return s\n",
    "\n",
    "def read_raw_data(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = [normalize_string(line) for line in f if normalize_string(line)]\n",
    "        print(f\"Successfully read {len(lines)} lines from {file_path}\")\n",
    "        return lines\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def download_file(url, dest_folder, filename):\n",
    "    os.makedirs(dest_folder, exist_ok=True)\n",
    "    zip_dest_path = os.path.join(dest_folder, filename) # Path to the zip file\n",
    "\n",
    "    if not os.path.exists(GLOVE_PATH): # Chỉ tải nếu file txt chưa tồn tại\n",
    "        if not os.path.exists(zip_dest_path):\n",
    "            print(f\"Downloading {filename} from {url}...\")\n",
    "            try:\n",
    "                response = requests.get(url, stream=True)\n",
    "                response.raise_for_status() # Check for download errors\n",
    "                total_size = int(response.headers.get('content-length', 0))\n",
    "                block_size = 1024 # 1 Kibibyte\n",
    "                t = tqdm(total=total_size, unit='iB', unit_scale=True, desc=f\"Downloading {filename}\")\n",
    "                with open(zip_dest_path + '.tmp', 'wb') as f: # Download to temp file\n",
    "                    for data in response.iter_content(block_size):\n",
    "                        t.update(len(data))\n",
    "                        f.write(data)\n",
    "                t.close()\n",
    "                if total_size != 0 and t.n != total_size:\n",
    "                     print(\"ERROR, something went wrong during download\")\n",
    "                     if os.path.exists(zip_dest_path + '.tmp'): os.remove(zip_dest_path + '.tmp')\n",
    "                     return False\n",
    "                os.rename(zip_dest_path + '.tmp', zip_dest_path) # Rename after successful download\n",
    "                print(f\"Downloaded {filename} successfully.\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error downloading {filename}: {e}\")\n",
    "                if os.path.exists(zip_dest_path + '.tmp'): os.remove(zip_dest_path + '.tmp') # Clean up temp file\n",
    "                return False\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred during download: {e}\")\n",
    "                if os.path.exists(zip_dest_path + '.tmp'): os.remove(zip_dest_path + '.tmp')\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"{filename} (zip file) already exists.\")\n",
    "\n",
    "        # --- Unzip ---\n",
    "        if os.path.exists(zip_dest_path) and zip_filename.endswith('.zip'):\n",
    "            print(f\"Attempting to unzip {filename}...\")\n",
    "            try:\n",
    "                with zipfile.ZipFile(zip_dest_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(dest_folder)\n",
    "                print(f\"Unzipped {filename} to {dest_folder}\")\n",
    "                # Check if the target file exists after unzipping\n",
    "                if not os.path.exists(GLOVE_PATH):\n",
    "                     print(f\"Error: Target GloVe file {GLOVE_FILENAME} not found after unzipping.\")\n",
    "                     return False\n",
    "                return True # Indicate success or file already exists\n",
    "            except zipfile.BadZipFile:\n",
    "                print(f\"Error: Downloaded file {filename} is not a valid zip file or is corrupted.\")\n",
    "                # Optionally remove corrupted zip file\n",
    "                # os.remove(zip_dest_path)\n",
    "                return False\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred during unzip: {e}\")\n",
    "                return False\n",
    "        elif not zip_filename.endswith('.zip'):\n",
    "             print(f\"Expected a zip file, but got {filename}\")\n",
    "             return False\n",
    "    else:\n",
    "         print(f\"{GLOVE_FILENAME} already exists. Skipping download/unzip.\")\n",
    "         return True # File already exists\n",
    "\n",
    "    return False # Should not reach here unless there's an issue\n",
    "\n",
    "\n",
    "# --- Tokenizer Training/Loading ---\n",
    "def train_or_load_tokenizer(lang, sentences, vocab_size):\n",
    "    tokenizer_path = os.path.join(TOKENIZER_DIR, f'{lang}_tokenizer.json')\n",
    "    os.makedirs(TOKENIZER_DIR, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(tokenizer_path):\n",
    "        print(f\"Loading existing tokenizer for {lang} from {tokenizer_path}\")\n",
    "        try:\n",
    "            tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading tokenizer from {tokenizer_path}: {e}\")\n",
    "            print(\"Attempting to retrain...\")\n",
    "            os.remove(tokenizer_path) # Remove corrupted file\n",
    "            return train_or_load_tokenizer(lang, sentences, vocab_size) # Retry training\n",
    "    else:\n",
    "        print(f\"Training tokenizer for {lang}...\")\n",
    "        tokenizer = Tokenizer(WordPiece(unk_token=UNK_TOKEN))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordPieceTrainer(vocab_size=vocab_size, special_tokens=SPECIAL_TOKENS)\n",
    "\n",
    "        # Filter out empty sentences before training tokenizer\n",
    "        non_empty_sentences = [s for s in sentences if s]\n",
    "        if not non_empty_sentences:\n",
    "             print(f\"Error: No sentences provided for training {lang} tokenizer.\")\n",
    "             return None\n",
    "\n",
    "        tokenizer.train_from_iterator(non_empty_sentences, trainer=trainer)\n",
    "\n",
    "        sos_token_id = tokenizer.token_to_id(SOS_TOKEN)\n",
    "        eos_token_id = tokenizer.token_to_id(EOS_TOKEN)\n",
    "\n",
    "        if sos_token_id is not None and eos_token_id is not None:\n",
    "            tokenizer.post_processor = TemplateProcessing(\n",
    "                single=f\"{SOS_TOKEN} $A {EOS_TOKEN}\",\n",
    "                special_tokens=[(SOS_TOKEN, sos_token_id), (EOS_TOKEN, eos_token_id)],\n",
    "            )\n",
    "            print(f\"Set post-processor for {lang} tokenizer.\")\n",
    "        else:\n",
    "            print(f\"Warning: SOS or EOS token not found in {lang} tokenizer vocab after training.\")\n",
    "\n",
    "        try:\n",
    "            tokenizer.save(tokenizer_path)\n",
    "            print(f\"Saved tokenizer for {lang} to {tokenizer_path}\")\n",
    "        except Exception as e:\n",
    "             print(f\"Error saving tokenizer for {lang} to {tokenizer_path}: {e}\")\n",
    "             return None # Indicate failure\n",
    "\n",
    "\n",
    "    pad_token_id = tokenizer.token_to_id(PAD_TOKEN)\n",
    "    if pad_token_id is not None:\n",
    "         tokenizer.enable_padding(pad_id=pad_token_id, pad_token=PAD_TOKEN, direction='right') # Ensure direction\n",
    "         print(f\"Enabled padding for {lang} tokenizer (PAD ID: {pad_token_id}).\")\n",
    "    else:\n",
    "        print(f\"Warning: {PAD_TOKEN} not found in {lang} tokenizer. Padding will not work.\")\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "# --- GloVe Loading ---\n",
    "def load_glove_embeddings(glove_path, embedding_dim, tokenizer):\n",
    "    print(f\"Loading GloVe embeddings from {glove_path}...\")\n",
    "    if not os.path.exists(glove_path):\n",
    "        print(f\"Error: GloVe file not found at {glove_path}\")\n",
    "        return None\n",
    "\n",
    "    word_to_idx = tokenizer.get_vocab() # Get word -> index mapping\n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "    print(f\"Tokenizer vocab size (for embedding matrix): {vocab_size}\")\n",
    "    embeddings = np.zeros((vocab_size, embedding_dim))\n",
    "    found_words = 0\n",
    "\n",
    "    try:\n",
    "        with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.split()\n",
    "                word = parts[0]\n",
    "                if word in word_to_idx: # Check if word from GloVe is in our tokenizer vocab\n",
    "                    try:\n",
    "                        vector = np.array(parts[1:], dtype=np.float32)\n",
    "                        if vector.shape[0] == embedding_dim: # Check dimension match\n",
    "                            embeddings[word_to_idx[word]] = vector\n",
    "                            found_words += 1\n",
    "                        # else: print(f\"Warning: Dim mismatch for '{word}'\") # Verbose\n",
    "                    except ValueError:\n",
    "                        # print(f\"Warning: Cannot parse vector for '{word}'\") # Verbose\n",
    "                        pass # Skip malformed lines\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading GloVe file: {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Loaded {found_words}/{len(word_to_idx)} words from GloVe file into embedding matrix.\")\n",
    "\n",
    "    # Handle special tokens (important!)\n",
    "    pad_idx = tokenizer.token_to_id(PAD_TOKEN)\n",
    "    unk_idx = tokenizer.token_to_id(UNK_TOKEN)\n",
    "    sos_idx = tokenizer.token_to_id(SOS_TOKEN)\n",
    "    eos_idx = tokenizer.token_to_id(EOS_TOKEN)\n",
    "\n",
    "    # It's crucial PAD is zeros for padding_idx to work correctly in nn.Embedding\n",
    "    if pad_idx is not None:\n",
    "        embeddings[pad_idx] = np.zeros(embedding_dim)\n",
    "        print(f\"Set PAD token embedding (Index: {pad_idx}) to zeros.\")\n",
    "\n",
    "    # Initialize UNK with small random values if not found in GloVe\n",
    "    if unk_idx is not None and np.all(embeddings[unk_idx] == 0):\n",
    "        print(f\"Initializing UNK token embedding (Index: {unk_idx}) randomly.\")\n",
    "        embeddings[unk_idx] = np.random.randn(embedding_dim) * 0.01\n",
    "\n",
    "    # Optionally initialize SOS/EOS if they weren't found\n",
    "    if sos_idx is not None and np.all(embeddings[sos_idx] == 0):\n",
    "        print(f\"Initializing SOS token embedding (Index: {sos_idx}) randomly.\")\n",
    "        embeddings[sos_idx] = np.random.randn(embedding_dim) * 0.01\n",
    "    if eos_idx is not None and np.all(embeddings[eos_idx] == 0):\n",
    "        print(f\"Initializing EOS token embedding (Index: {eos_idx}) randomly.\")\n",
    "        embeddings[eos_idx] = np.random.randn(embedding_dim) * 0.01\n",
    "\n",
    "    return torch.tensor(embeddings, dtype=torch.float)\n",
    "\n",
    "\n",
    "# --- Dataset using Hugging Face Tokenizer ---\n",
    "class TranslationDatasetHF(Dataset):\n",
    "    def __init__(self, src_sentences, trg_sentences, src_tokenizer, trg_tokenizer):\n",
    "        if not src_sentences or not trg_sentences:\n",
    "            raise ValueError(\"Source or target sentences list is empty.\")\n",
    "        if len(src_sentences) != len(trg_sentences):\n",
    "             raise ValueError(\"Source and target sentences must have the same length.\")\n",
    "\n",
    "        self.src_sentences = src_sentences\n",
    "        self.trg_sentences = trg_sentences\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.src_sentences[idx]\n",
    "        trg_text = self.trg_sentences[idx]\n",
    "\n",
    "        # Encode text using the tokenizer (should add SOS/EOS if post_processor is set)\n",
    "        src_encoded = self.src_tokenizer.encode(src_text)\n",
    "        trg_encoded = self.trg_tokenizer.encode(trg_text)\n",
    "\n",
    "        src_ids = src_encoded.ids\n",
    "        trg_ids = trg_encoded.ids\n",
    "\n",
    "        # Basic check if encoding produced empty lists (might happen with empty input strings)\n",
    "        if not src_ids: src_ids = [self.src_tokenizer.token_to_id(PAD_TOKEN)] # Handle potential empty encoding\n",
    "        if not trg_ids: trg_ids = [self.trg_tokenizer.token_to_id(PAD_TOKEN)]\n",
    "\n",
    "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(trg_ids, dtype=torch.long)\n",
    "\n",
    "# --- Collate Function for HF Tokenizer Output ---\n",
    "def collate_fn_hf(batch, pad_idx_src, pad_idx_trg):\n",
    "    src_batch, trg_batch = [], []\n",
    "    for src_item, trg_item in batch:\n",
    "        src_batch.append(src_item)\n",
    "        trg_batch.append(trg_item)\n",
    "\n",
    "    # Pad sequences to the max length in the batch\n",
    "    src_batch_padded = pad_sequence(src_batch, batch_first=True, padding_value=pad_idx_src)\n",
    "    trg_batch_padded = pad_sequence(trg_batch, batch_first=True, padding_value=pad_idx_trg)\n",
    "\n",
    "    return src_batch_padded, trg_batch_padded\n",
    "\n",
    "# --- Model Definition (Encoder adapted for GloVe) ---\n",
    "class EncoderGRU(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers, dropout, embedding_weights=None, freeze_emb=True, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        if embedding_weights is not None:\n",
    "            print(\"Encoder: Initializing embedding layer with pre-trained weights.\")\n",
    "            # Ensure emb_dim matches pre-trained dimension\n",
    "            if emb_dim != embedding_weights.shape[1]:\n",
    "                 print(f\"Warning: emb_dim ({emb_dim}) does not match pre-trained embedding dimension ({embedding_weights.shape[1]}). Adjusting emb_dim.\")\n",
    "                 emb_dim = embedding_weights.shape[1] # Override emb_dim\n",
    "\n",
    "            self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=freeze_emb, padding_idx=pad_idx)\n",
    "            print(f\"Encoder embedding layer created with shape: {self.embedding.weight.shape}, Frozen: {freeze_emb}\")\n",
    "        else:\n",
    "            print(\"Encoder: Initializing embedding layer randomly.\")\n",
    "            self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n",
    "            print(f\"Encoder embedding layer created with shape: {self.embedding.weight.shape}\")\n",
    "\n",
    "\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, num_layers,\n",
    "                          dropout=dropout if num_layers > 1 else 0,\n",
    "                          batch_first=True, bidirectional=False) # Set bidirectional=False explicitly\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src_seq):\n",
    "        # src_seq shape: [batch_size, src_len]\n",
    "        embedded = self.dropout(self.embedding(src_seq))\n",
    "        # embedded shape: [batch_size, src_len, emb_dim]\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        # If GRU is bidirectional, hidden shape is [num_layers*2, batch_size, hidden_dim].\n",
    "        # Need to handle this if using bidirectional. For now, assuming unidirectional.\n",
    "        # hidden shape: [num_layers, batch_size, hidden_dim]\n",
    "        return hidden\n",
    "\n",
    "class DecoderGRU(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, num_layers, dropout, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        print(\"Decoder: Initializing embedding layer randomly.\")\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=pad_idx)\n",
    "        print(f\"Decoder embedding layer created with shape: {self.embedding.weight.shape}\")\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, num_layers,\n",
    "                          dropout=dropout if num_layers > 1 else 0,\n",
    "                          batch_first=True) # batch_first=True\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_step, hidden_state):\n",
    "        # input_step shape: [batch_size] -> [batch_size, 1]\n",
    "        input_step = input_step.unsqueeze(1)\n",
    "        # embedded shape: [batch_size, 1, emb_dim]\n",
    "        embedded = self.dropout(self.embedding(input_step))\n",
    "        # output shape: [batch_size, 1, hidden_dim]\n",
    "        # new_hidden_state shape: [num_layers, batch_size, hidden_dim]\n",
    "        output, new_hidden_state = self.gru(embedded, hidden_state)\n",
    "        # output shape: [batch_size, hidden_dim]\n",
    "        output = output.squeeze(1)\n",
    "        # prediction shape: [batch_size, output_dim]\n",
    "        prediction = self.fc_out(output)\n",
    "        return prediction, new_hidden_state\n",
    "\n",
    "# Seq2Seq Model (No changes needed)\n",
    "class Seq2SeqGRU(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        # Assert dimensions match\n",
    "        assert encoder.hidden_dim == decoder.hidden_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.num_layers == decoder.num_layers, \\\n",
    "            \"Number of layers in encoder and decoder must be equal!\"\n",
    "\n",
    "    def forward(self, src_seq, trg_seq, teacher_forcing_ratio=0.5):\n",
    "        batch_size = trg_seq.shape[0]\n",
    "        trg_len = trg_seq.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # Tensor to store decoder outputs\n",
    "        decoder_outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # Encoder forward pass\n",
    "        encoder_hidden = self.encoder(src_seq) # [num_layers, batch_size, hidden_dim]\n",
    "\n",
    "        # Initialize decoder hidden state with encoder's final hidden state\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        # First input to the decoder is the <sos> tokens from the target sequence\n",
    "        decoder_input = trg_seq[:, 0] # Shape: [batch_size]\n",
    "\n",
    "        # Loop through the target sequence length\n",
    "        for t in range(1, trg_len): # Start from 1 as 0 is <sos>\n",
    "            # Decoder forward pass for one time step\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "            # Store prediction\n",
    "            decoder_outputs[:, t, :] = decoder_output\n",
    "\n",
    "            # Decide whether to use teacher forcing\n",
    "            use_teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            if use_teacher_force:\n",
    "                # Use actual next token from target sequence as next input\n",
    "                decoder_input = trg_seq[:, t]\n",
    "            else:\n",
    "                # Use decoder's own prediction as next input\n",
    "                top1 = decoder_output.argmax(1) # Get index of highest probability token\n",
    "                decoder_input = top1\n",
    "                # Check if all sequences predicted EOS (optimization: break early)\n",
    "                # if (decoder_input == EOS_IDX_VI).all():\n",
    "                #    break\n",
    "\n",
    "        return decoder_outputs\n",
    "\n",
    "\n",
    "# --- Weight Initialization ---\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        # Skip pre-trained embedding layer in encoder\n",
    "        if 'encoder.embedding.weight' in name and glove_embeddings is not None:\n",
    "             print(f\"Skipping initialization for {name} (pre-trained)\")\n",
    "             continue\n",
    "        # Initialize other weights\n",
    "        if param.dim() > 1:\n",
    "            nn.init.xavier_uniform_(param)\n",
    "        # Initialize biases to zero\n",
    "        elif 'bias' in name:\n",
    "            nn.init.constant_(param, 0)\n",
    "\n",
    "# --- Training Loop Function ---\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    processed_batches = 0\n",
    "    print(f\"Starting training epoch...\")\n",
    "    batch_progress = tqdm(iterator, desc=\"Training\", leave=False)\n",
    "    for i, batch in enumerate(batch_progress):\n",
    "        try:\n",
    "            src, trg = batch\n",
    "            src = src.to(DEVICE)\n",
    "            trg = trg.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # output shape: [batch_size, trg_len, output_vocab_size]\n",
    "            output = model(src, trg, teacher_forcing_ratio=TEACHER_FORCING_RATIO)\n",
    "\n",
    "            # Reshape for CrossEntropyLoss:\n",
    "            # Output: remove <sos> token -> [batch_size, trg_len-1, output_vocab_size] -> [(batch_size*(trg_len-1)), output_vocab_size]\n",
    "            # Target: remove <sos> token -> [batch_size, trg_len-1] -> [(batch_size*(trg_len-1))]\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            if torch.isnan(loss):\n",
    "                print(f\"Warning: NaN loss detected at batch {i}. Skipping update.\")\n",
    "                continue # Skip this batch\n",
    "\n",
    "            loss.backward()\n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            processed_batches += 1\n",
    "            batch_progress.set_postfix(loss=loss.item())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during training batch {i}: {e}\")\n",
    "            print(\"Skipping this batch.\")\n",
    "            traceback.print_exc() # Print detailed error\n",
    "            optimizer.zero_grad() # Ensure grads are cleared\n",
    "            continue # Move to next batch\n",
    "\n",
    "    if processed_batches == 0:\n",
    "        print(\"Warning: No batches were processed successfully in this training epoch.\")\n",
    "        return float('inf') # Return infinity if no batches processed\n",
    "\n",
    "    return epoch_loss / processed_batches\n",
    "\n",
    "# --- Evaluation Loop Function ---\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    processed_batches = 0\n",
    "    print(f\"Starting evaluation...\")\n",
    "    batch_progress = tqdm(iterator, desc=\"Evaluating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(batch_progress):\n",
    "            try:\n",
    "                src, trg = batch\n",
    "                src = src.to(DEVICE)\n",
    "                trg = trg.to(DEVICE)\n",
    "\n",
    "                # Turn off teacher forcing for evaluation\n",
    "                output = model(src, trg, teacher_forcing_ratio=0)\n",
    "\n",
    "                output_dim = output.shape[-1]\n",
    "                output = output[:, 1:].reshape(-1, output_dim)\n",
    "                trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "                loss = criterion(output, trg)\n",
    "                if torch.isnan(loss):\n",
    "                    print(f\"Warning: NaN loss detected during evaluation batch {i}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                processed_batches += 1\n",
    "                batch_progress.set_postfix(loss=loss.item())\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError during evaluation batch {i}: {e}\")\n",
    "                print(\"Skipping this batch.\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "    if processed_batches == 0:\n",
    "         print(\"Warning: No batches were processed successfully in evaluation.\")\n",
    "         return float('inf')\n",
    "\n",
    "    return epoch_loss / processed_batches\n",
    "\n",
    "# --- Helper function for timing ---\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "# --- Inference Function ---\n",
    "def translate_sentence_hf(sentence: str, src_tokenizer: Tokenizer, trg_tokenizer: Tokenizer, model: Seq2SeqGRU, device, max_len=50):\n",
    "    model.eval()\n",
    "\n",
    "    if not isinstance(sentence, str) or not sentence.strip():\n",
    "        return \"Input sentence is empty or invalid.\"\n",
    "\n",
    "    # Tiền xử lý câu nguồn\n",
    "    cleaned_sentence = normalize_string(sentence)\n",
    "    src_encoded = src_tokenizer.encode(cleaned_sentence)\n",
    "    if not src_encoded or not src_encoded.ids:\n",
    "        return \"Input sentence is empty after tokenization.\"\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_encoded.ids).unsqueeze(0).to(device)\n",
    "\n",
    "    trg_sos_id = trg_tokenizer.token_to_id(SOS_TOKEN)\n",
    "    trg_eos_id = trg_tokenizer.token_to_id(EOS_TOKEN)\n",
    "    if trg_sos_id is None or trg_eos_id is None:\n",
    "        print(\"Error: Target SOS or EOS token ID not found in tokenizer.\")\n",
    "        return \"Error during translation setup.\"\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            encoder_hidden = model.encoder(src_tensor)\n",
    "            decoder_hidden = encoder_hidden\n",
    "            decoder_input = torch.LongTensor([trg_sos_id]).to(device)\n",
    "            trg_ids_result = []\n",
    "\n",
    "            for _ in range(max_len):\n",
    "                output, decoder_hidden = model.decoder(decoder_input, decoder_hidden)\n",
    "                pred_token_id = output.argmax(1).item()\n",
    "\n",
    "                if pred_token_id == trg_eos_id:\n",
    "                    break # Stop if EOS is predicted\n",
    "\n",
    "                trg_ids_result.append(pred_token_id)\n",
    "                decoder_input = torch.LongTensor([pred_token_id]).to(device)\n",
    "\n",
    "        # Decode the result (Hugging Face decode often handles special tokens)\n",
    "        translated_text = trg_tokenizer.decode(trg_ids_result, skip_special_tokens=True)\n",
    "        return translated_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during translation inference: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return \"Error during translation.\"\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "#                       MAIN EXECUTION BLOCK\n",
    "# ================================================================\n",
    "\n",
    "if __name__ == '__main__': # Ensure this runs only when script is executed directly\n",
    "\n",
    "    # 1. Download GloVe data if necessary\n",
    "    if not download_file(GLOVE_ZIP_URL, GLOVE_DIR, GLOVE_ZIP_FILENAME):\n",
    "        print(\"GloVe preparation failed. Please ensure the GloVe file exists or download is possible.\")\n",
    "        exit()\n",
    "\n",
    "    # 2. Load Raw Data\n",
    "    print(\"\\n--- Loading Raw Data ---\")\n",
    "    en_sents_raw = read_raw_data(EN_FILE)\n",
    "    vi_sents_raw = read_raw_data(VI_FILE)\n",
    "\n",
    "    if en_sents_raw is None or vi_sents_raw is None or len(en_sents_raw) != len(vi_sents_raw):\n",
    "        print(\"Exiting due to data loading error or length mismatch.\")\n",
    "        exit()\n",
    "    if not en_sents_raw:\n",
    "         print(\"Error: No data loaded. Check data files.\")\n",
    "         exit()\n",
    "\n",
    "    # 3. Split Data\n",
    "    print(\"\\n--- Splitting Data ---\")\n",
    "    combined = list(zip(en_sents_raw, vi_sents_raw))\n",
    "    random.shuffle(combined) # Shuffle before splitting\n",
    "    train_size = int(0.8 * len(combined))\n",
    "    valid_size = int(0.1 * len(combined))\n",
    "    # Ensure sizes are valid\n",
    "    if train_size == 0 or valid_size == 0 or len(combined) - train_size - valid_size == 0:\n",
    "        print(\"Error: Dataset too small to split into train/validation/test sets.\")\n",
    "        # Handle small datasets differently, e.g., use only train/val or cross-validation\n",
    "        # For this example, we'll exit if the split is invalid\n",
    "        exit()\n",
    "\n",
    "    train_data = combined[:train_size]\n",
    "    valid_data = combined[train_size:train_size+valid_size]\n",
    "    test_data = combined[train_size+valid_size:]\n",
    "\n",
    "    en_train_sents, vi_train_sents = zip(*train_data)\n",
    "    en_valid_sents, vi_valid_sents = zip(*valid_data)\n",
    "    en_test_sents, vi_test_sents = zip(*test_data)\n",
    "\n",
    "    print(f\"Train size: {len(en_train_sents)}\")\n",
    "    print(f\"Validation size: {len(en_valid_sents)}\")\n",
    "    print(f\"Test size: {len(en_test_sents)}\")\n",
    "\n",
    "\n",
    "    # 4. Train/Load Tokenizers\n",
    "    print(\"\\n--- Preparing Tokenizers ---\")\n",
    "    tokenizer_en = train_or_load_tokenizer(\"en\", en_train_sents, vocab_size=MAX_VOCAB_SIZE)\n",
    "    tokenizer_vi = train_or_load_tokenizer(\"vi\", vi_train_sents, vocab_size=MAX_VOCAB_SIZE)\n",
    "\n",
    "    if tokenizer_en is None or tokenizer_vi is None:\n",
    "         print(\"Exiting due to tokenizer preparation error.\")\n",
    "         exit()\n",
    "\n",
    "    INPUT_VOCAB_SIZE = tokenizer_en.get_vocab_size()\n",
    "    OUTPUT_VOCAB_SIZE = tokenizer_vi.get_vocab_size()\n",
    "    PAD_IDX_EN = tokenizer_en.token_to_id(PAD_TOKEN)\n",
    "    PAD_IDX_VI = tokenizer_vi.token_to_id(PAD_TOKEN)\n",
    "\n",
    "    # Check if PAD tokens exist, otherwise padding will fail\n",
    "    if PAD_IDX_EN is None or PAD_IDX_VI is None:\n",
    "        print(f\"Error: PAD token ('{PAD_TOKEN}') not found in one or both tokenizers. Cannot proceed.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"English Vocab Size: {INPUT_VOCAB_SIZE}\")\n",
    "    print(f\"Vietnamese Vocab Size: {OUTPUT_VOCAB_SIZE}\")\n",
    "\n",
    "\n",
    "    # 5. Load GloVe Embeddings\n",
    "    print(\"\\n--- Loading GloVe Embeddings ---\")\n",
    "    glove_embeddings = load_glove_embeddings(GLOVE_PATH, EMBEDDING_DIM_GLOVE, tokenizer_en)\n",
    "    if glove_embeddings is None:\n",
    "         print(\"Warning: Failed to load GloVe embeddings. Encoder embedding will be random.\")\n",
    "         # Decide if you want to exit or continue with random embeddings\n",
    "         # exit()\n",
    "\n",
    "\n",
    "    # 6. Create Datasets and DataLoaders\n",
    "    print(\"\\n--- Creating Datasets and DataLoaders ---\")\n",
    "    try:\n",
    "        train_dataset = TranslationDatasetHF(en_train_sents, vi_train_sents, tokenizer_en, tokenizer_vi)\n",
    "        valid_dataset = TranslationDatasetHF(en_valid_sents, vi_valid_sents, tokenizer_en, tokenizer_vi)\n",
    "        test_dataset  = TranslationDatasetHF(en_test_sents, vi_test_sents, tokenizer_en, tokenizer_vi)\n",
    "    except ValueError as e:\n",
    "         print(f\"Error creating dataset: {e}\")\n",
    "         exit()\n",
    "\n",
    "\n",
    "    collate_with_padding_hf = lambda batch: collate_fn_hf(batch, PAD_IDX_EN, PAD_IDX_VI)\n",
    "\n",
    "    train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_with_padding_hf)\n",
    "    valid_iterator = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_with_padding_hf)\n",
    "    test_iterator  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_with_padding_hf)\n",
    "    print(\"DataLoaders created.\")\n",
    "\n",
    "\n",
    "    # 7. Initialize Model, Optimizer, Criterion\n",
    "    print(\"\\n--- Initializing Model ---\")\n",
    "    encoder = EncoderGRU(INPUT_VOCAB_SIZE, EMBEDDING_DIM_GLOVE, HIDDEN_DIM, NUM_LAYERS, DROPOUT_RATE,\n",
    "                         embedding_weights=glove_embeddings, freeze_emb=FREEZE_GLOVE, pad_idx=PAD_IDX_EN).to(DEVICE)\n",
    "    # Decoder embedding dim can be different if needed\n",
    "    decoder = DecoderGRU(OUTPUT_VOCAB_SIZE, EMBEDDING_DIM_VI, HIDDEN_DIM, NUM_LAYERS, DROPOUT_RATE,\n",
    "                         pad_idx=PAD_IDX_VI).to(DEVICE)\n",
    "    model = Seq2SeqGRU(encoder, decoder, DEVICE).to(DEVICE)\n",
    "\n",
    "    # Apply custom weight initialization\n",
    "    model.apply(init_weights)\n",
    "    print(\"Model initialized.\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX_VI)\n",
    "    print(\"Optimizer and Criterion set.\")\n",
    "\n",
    "\n",
    "    # --- Training Execution ---\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    print(\"\\n--- Starting Training Loop ---\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        print(f\"\\nEpoch: {epoch+1:02}/{NUM_EPOCHS}\")\n",
    "        train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "        valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\"\\t-> Saved Best Model (Val Loss: {valid_loss:.3f})\")\n",
    "        else:\n",
    "            print(f\"\\t   Validation loss did not improve from {best_valid_loss:.3f}\")\n",
    "\n",
    "        print(f'\\tTime: {epoch_mins}m {epoch_secs}s')\n",
    "        # Handle potential inf loss from train/eval loops if no batches were processed\n",
    "        if train_loss != float('inf'):\n",
    "            print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(min(train_loss, 700)):7.3f}') # Cap PPL for display\n",
    "        else:\n",
    "             print('\\tTrain Loss: Inf (No batches processed)')\n",
    "        if valid_loss != float('inf'):\n",
    "            print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(min(valid_loss, 700)):7.3f}') # Cap PPL for display\n",
    "        else:\n",
    "             print('\\t Val. Loss: Inf (No batches processed)')\n",
    "\n",
    "\n",
    "    print(\"\\n--- Training Finished ---\")\n",
    "\n",
    "\n",
    "    # --- Load Best Model and Translate Examples ---\n",
    "    print(f\"\\n--- Loading Best Model for Inference ({MODEL_SAVE_PATH}) ---\")\n",
    "    try:\n",
    "        # Load the state dict onto the correct device\n",
    "        model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
    "        print(\"Model loaded successfully.\")\n",
    "\n",
    "        # Example 1: Custom Sentence\n",
    "        example_sentence_en = \"a man in a blue shirt is playing a guitar .\" # Example sentence\n",
    "        print(f\"\\nSource EN: {example_sentence_en}\")\n",
    "        translation = translate_sentence_hf(example_sentence_en, tokenizer_en, tokenizer_vi, model, DEVICE)\n",
    "        print(f'Predicted VI: {translation}')\n",
    "\n",
    "        # Example 2: Sentence from Test Set\n",
    "        if test_data: # Check if test set is not empty\n",
    "            print(\"\\nTranslating a sentence from the test set:\")\n",
    "            test_idx = random.randint(0, len(en_test_sents) - 1)\n",
    "            src_test_sent = en_test_sents[test_idx]\n",
    "            trg_test_sent = vi_test_sents[test_idx] # Ground truth for comparison\n",
    "            print(f\"Source EN: {src_test_sent}\")\n",
    "            print(f\"Actual VI: {trg_test_sent}\")\n",
    "            translation_test = translate_sentence_hf(src_test_sent, tokenizer_en, tokenizer_vi, model, DEVICE)\n",
    "            print(f'Predicted VI: {translation_test}')\n",
    "        else:\n",
    "             print(\"\\nTest set is empty, cannot translate test example.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nModel file '{MODEL_SAVE_PATH}' not found. Cannot perform inference.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during final inference: {e}\")\n",
    "        traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vy_minh_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
