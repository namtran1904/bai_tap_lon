{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "092a4a87",
   "metadata": {},
   "source": [
    "## import định nghĩa các hằng sốs, tham số"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e5f8122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Batch Size: 16\n",
      "Num Epochs: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "import zipfile\n",
    "import traceback\n",
    "\n",
    "# --- Constants ---\n",
    "DATA_DIR = 'data'\n",
    "EN_FILE = os.path.join(DATA_DIR, 'en_sents')\n",
    "VI_FILE = os.path.join(DATA_DIR, 'vi_sents')\n",
    "TOKENIZER_DIR = 'tokenizers'\n",
    "GLOVE_DIR = 'glove_data'\n",
    "GLOVE_ZIP_URL = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "GLOVE_ZIP_FILENAME = 'glove.6B.zip'\n",
    "GLOVE_FILENAME = 'glove.6B.300d.txt' \n",
    "GLOVE_PATH = os.path.join(GLOVE_DIR, GLOVE_FILENAME)\n",
    "MODEL_SAVE_PATH = 'seq2seq-gru-bidir-glove-hf.pt'\n",
    "\n",
    "# Special tokens\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "SOS_TOKEN = \"[SOS]\"\n",
    "EOS_TOKEN = \"[EOS]\"\n",
    "SPECIAL_TOKENS = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "EMBEDDING_DIM_GLOVE = 300\n",
    "EMBEDDING_DIM_VI = 256\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 2 # Số lớp cho mỗi chiều GRU Encoder và cho GRU Decoder\n",
    "DROPOUT_RATE = 0.3\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 16 \n",
    "NUM_EPOCHS = 2  \n",
    "CLIP = 1.0\n",
    "TEACHER_FORCING_RATIO = 0.5\n",
    "FREEZE_GLOVE = True\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Num Epochs: {NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a845ce",
   "metadata": {},
   "source": [
    "## Định nghĩa các hàm tiện ích"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce933355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions, Tokenizer, GloVe loader, Dataset defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Utility Functions ---\n",
    "def normalize_string(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def read_raw_data(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = [clean_s for s in f if (clean_s := normalize_string(s))]\n",
    "        print(f\"Successfully read {len(lines)} lines from {file_path}\")\n",
    "        return lines\n",
    "    except FileNotFoundError: print(f\"Error: File not found: {file_path}\"); return None\n",
    "    except Exception as e: print(f\"Error reading {file_path}: {e}\"); return None\n",
    "\n",
    "def download_file(url, dest_folder, filename):\n",
    "    os.makedirs(dest_folder, exist_ok=True)\n",
    "    zip_dest_path = os.path.join(dest_folder, filename)\n",
    "    if not os.path.exists(GLOVE_PATH):\n",
    "        if not os.path.exists(zip_dest_path):\n",
    "            print(f\"Downloading {filename} from {url}...\")\n",
    "            try:\n",
    "                response = requests.get(url, stream=True, timeout=60) # Tăng timeout\n",
    "                response.raise_for_status()\n",
    "                total_size = int(response.headers.get('content-length', 0))\n",
    "                t = tqdm(total=total_size, unit='iB', unit_scale=True, desc=f\"Downloading {filename}\")\n",
    "                with open(zip_dest_path + '.tmp', 'wb') as f:\n",
    "                    for data in response.iter_content(1024*10): t.update(len(data)); f.write(data) # Tăng block size\n",
    "                t.close()\n",
    "                if total_size != 0 and t.n != total_size: print(f\"ERROR: DL incomplete.\"); os.remove(zip_dest_path + '.tmp'); return False\n",
    "                os.rename(zip_dest_path + '.tmp', zip_dest_path); print(f\"Downloaded {filename}.\")\n",
    "            except Exception as e: print(f\"Download error: {e}\"); return False\n",
    "        else: print(f\"{filename} (zip) exists.\")\n",
    "        if os.path.exists(zip_dest_path) and filename.endswith('.zip'):\n",
    "            print(f\"Unzipping {filename}...\")\n",
    "            try:\n",
    "                with zipfile.ZipFile(zip_dest_path, 'r') as zf:\n",
    "                    if GLOVE_FILENAME in zf.namelist(): zf.extract(GLOVE_FILENAME, dest_folder)\n",
    "                    else: print(f\"Warn: {GLOVE_FILENAME} not in zip. Extract all...\"); zf.extractall(dest_folder)\n",
    "                if not os.path.exists(GLOVE_PATH): print(f\"Error: {GLOVE_FILENAME} not found after unzip.\"); return False\n",
    "                print(\"Unzip successful.\"); return True\n",
    "            except Exception as e: print(f\"Unzip error: {e}\"); return False\n",
    "        elif not filename.endswith('.zip'): print(f\"Expected zip, got {filename}\"); return False\n",
    "    else: print(f\"{GLOVE_FILENAME} exists.\"); return True\n",
    "    return False\n",
    "\n",
    "# --- Tokenizer Training/Loading ---\n",
    "def train_or_load_tokenizer(lang, sentences_iterator, vocab_size, min_frequency=2):\n",
    "    tokenizer_path = os.path.join(TOKENIZER_DIR, f'{lang}_tokenizer.json')\n",
    "    os.makedirs(TOKENIZER_DIR, exist_ok=True)\n",
    "    if os.path.exists(tokenizer_path):\n",
    "        print(f\"Loading tokenizer for {lang} from {tokenizer_path}\")\n",
    "        try: tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "        except Exception as e: print(f\"Error loading tokenizer: {e}. Retraining...\"); os.remove(tokenizer_path); return train_or_load_tokenizer(lang, sentences_iterator, vocab_size, min_frequency)\n",
    "    else:\n",
    "        print(f\"Training tokenizer for {lang}...\")\n",
    "        tokenizer = Tokenizer(WordPiece(unk_token=UNK_TOKEN)); tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordPieceTrainer(vocab_size=vocab_size, special_tokens=SPECIAL_TOKENS, min_frequency=min_frequency)\n",
    "        try:\n",
    "             sentence_list = list(sentences_iterator) # Cần list để lấy length\n",
    "             if not sentence_list: print(f\"No sentences for {lang} tokenizer.\"); return None\n",
    "             tokenizer.train_from_iterator(sentence_list, trainer=trainer, length=len(sentence_list))\n",
    "        except Exception as e: print(f\"Error during {lang} tokenizer training: {e}\"); return None\n",
    "        sos_id, eos_id = tokenizer.token_to_id(SOS_TOKEN), tokenizer.token_to_id(EOS_TOKEN)\n",
    "        if sos_id is not None and eos_id is not None:\n",
    "            tokenizer.post_processor = TemplateProcessing(single=f\"{SOS_TOKEN} $A {EOS_TOKEN}\", special_tokens=[(SOS_TOKEN, sos_id), (EOS_TOKEN, eos_id)]); print(f\"Set post-processor for {lang}.\")\n",
    "        else: print(f\"Warning: SOS/EOS not in {lang} vocab.\")\n",
    "        try: tokenizer.save(tokenizer_path); print(f\"Saved tokenizer for {lang}.\")\n",
    "        except Exception as e: print(f\"Error saving tokenizer: {e}\"); return None\n",
    "    pad_id = tokenizer.token_to_id(PAD_TOKEN)\n",
    "    if pad_id is not None: tokenizer.enable_padding(pad_id=pad_id, pad_token=PAD_TOKEN, direction='right'); print(f\"Enabled padding for {lang} (ID: {pad_id}).\")\n",
    "    else: print(f\"Warning: {PAD_TOKEN} not in {lang} tokenizer.\")\n",
    "    return tokenizer\n",
    "\n",
    "# --- GloVe Loading ---\n",
    "def load_glove_embeddings(glove_path, embedding_dim, tokenizer):\n",
    "    \"\"\"Loads GloVe embeddings into a tensor compatible with nn.Embedding.\"\"\"\n",
    "    print(f\"Loading GloVe embeddings from {glove_path}...\")\n",
    "    if not os.path.exists(glove_path):\n",
    "        print(f\"Error: GloVe file not found at {glove_path}\")\n",
    "        return None\n",
    "\n",
    "    word_to_idx = tokenizer.get_vocab() # Get word -> index mapping from trained tokenizer\n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "    print(f\"Tokenizer vocab size (for embedding matrix): {vocab_size}\")\n",
    "    # Initialize embedding matrix with zeros\n",
    "    embeddings = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "    found_words = 0\n",
    "\n",
    "    try:\n",
    "        line_count = 0 # Đếm dòng để theo dõi tiến trình\n",
    "        print(\"Reading GloVe file (this may take a while)...\")\n",
    "        with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "            # ---- THAY ĐỔI Ở ĐÂY: Bỏ tqdm ----\n",
    "            for line in f:\n",
    "            # ----------------------------------\n",
    "                line_count += 1\n",
    "                if line_count % 100000 == 0: # In tiến trình sau mỗi 100k dòng\n",
    "                    print(f\"  Processed {line_count} lines...\")\n",
    "\n",
    "                parts = line.split()\n",
    "                if len(parts) < embedding_dim + 1: # Basic check for malformed lines\n",
    "                    continue\n",
    "                word = parts[0]\n",
    "                if word in word_to_idx: # Check if word from GloVe is in our tokenizer vocab\n",
    "                    try:\n",
    "                        vector = np.array(parts[1:], dtype=np.float32)\n",
    "                        # Double-check dimension after conversion\n",
    "                        if vector.shape[0] == embedding_dim:\n",
    "                            embeddings[word_to_idx[word]] = vector\n",
    "                            found_words += 1\n",
    "                        # else: print(f\"Glove line {line_num+1}: Dim mismatch for '{word}'. Expected {embedding_dim}, Got {vector.shape[0]}\")\n",
    "                    except ValueError:\n",
    "                        # print(f\"Glove line {line_num+1}: Cannot parse vector for '{word}'\")\n",
    "                        pass # Skip if vector part is not numeric\n",
    "        print(f\"Finished reading {line_count} lines.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading GloVe file: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "    print(f\"Loaded {found_words}/{len(word_to_idx)} words from GloVe file into embedding matrix.\")\n",
    "\n",
    "    # --- Initialize embeddings for special tokens ---\n",
    "    pad_idx = tokenizer.token_to_id(PAD_TOKEN)\n",
    "    unk_idx = tokenizer.token_to_id(UNK_TOKEN)\n",
    "    sos_idx = tokenizer.token_to_id(SOS_TOKEN)\n",
    "    eos_idx = tokenizer.token_to_id(EOS_TOKEN)\n",
    "\n",
    "    # PAD token MUST be zeros if using padding_idx in nn.Embedding\n",
    "    if pad_idx is not None:\n",
    "        embeddings[pad_idx] = np.zeros(embedding_dim)\n",
    "        print(f\"Set PAD token embedding (Index: {pad_idx}) to zeros.\")\n",
    "\n",
    "    # Initialize UNK with small random values if not found in GloVe\n",
    "    if unk_idx is not None and np.all(embeddings[unk_idx] == 0):\n",
    "        print(f\"Initializing UNK token embedding (Index: {unk_idx}) randomly.\")\n",
    "        embeddings[unk_idx] = np.random.randn(embedding_dim) * 0.01\n",
    "\n",
    "    # Optionally initialize SOS/EOS if they weren't found or are zeros\n",
    "    if sos_idx is not None and np.all(embeddings[sos_idx] == 0):\n",
    "        print(f\"Initializing SOS token embedding (Index: {sos_idx}) randomly.\")\n",
    "        embeddings[sos_idx] = np.random.randn(embedding_dim) * 0.01\n",
    "    if eos_idx is not None and np.all(embeddings[eos_idx] == 0):\n",
    "        print(f\"Initializing EOS token embedding (Index: {eos_idx}) randomly.\")\n",
    "        embeddings[eos_idx] = np.random.randn(embedding_dim) * 0.01\n",
    "\n",
    "    return torch.tensor(embeddings, dtype=torch.float)\n",
    "\n",
    "# --- Dataset and Collate Function ---\n",
    "class TranslationDatasetHF(Dataset):\n",
    "    def __init__(self, src_sentences, trg_sentences, src_tokenizer: Tokenizer, trg_tokenizer: Tokenizer):\n",
    "        if not src_sentences or not trg_sentences: raise ValueError(\"Sentences list empty.\")\n",
    "        if len(src_sentences) != len(trg_sentences): raise ValueError(\"Sentence lists length mismatch.\")\n",
    "        self.src_s, self.trg_s, self.src_tok, self.trg_tok = src_sentences, trg_sentences, src_tokenizer, trg_tokenizer\n",
    "        self.pad_id_src = src_tokenizer.token_to_id(PAD_TOKEN) # Lưu lại để xử lý getitem\n",
    "        self.pad_id_trg = trg_tokenizer.token_to_id(PAD_TOKEN)\n",
    "    def __len__(self): return len(self.src_s)\n",
    "    def __getitem__(self, idx):\n",
    "        src_enc = self.src_tok.encode(self.src_s[idx]); trg_enc = self.trg_tok.encode(self.trg_s[idx])\n",
    "        src_ids = src_enc.ids if src_enc and src_enc.ids else [self.pad_id_src]\n",
    "        trg_ids = trg_enc.ids if trg_enc and trg_enc.ids else [self.pad_id_trg]\n",
    "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(trg_ids, dtype=torch.long)\n",
    "\n",
    "def collate_fn_hf(batch, pad_idx_src, pad_idx_trg):\n",
    "    src_b, trg_b = [], []\n",
    "    for src_i, trg_i in batch: src_b.append(src_i); trg_b.append(trg_i)\n",
    "    return pad_sequence(src_b, batch_first=True, padding_value=pad_idx_src), \\\n",
    "           pad_sequence(trg_b, batch_first=True, padding_value=pad_idx_trg)\n",
    "\n",
    "print(\"Utility functions, Tokenizer, GloVe loader, Dataset defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bbdc6c",
   "metadata": {},
   "source": [
    "## Định nghĩa các lớp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "210cafd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model classes (EncoderGRU, DecoderGRU, Seq2SeqGRU) defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Model Definition (Encoder with Bidirectional GRU) ---\n",
    "class EncoderGRU(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers, dropout,\n",
    "                 embedding_weights=None, freeze_emb=True, pad_idx=0):\n",
    "        super().__init__(); self.hidden_dim, self.num_layers, self.actual_emb_dim = hidden_dim, num_layers, emb_dim\n",
    "        if embedding_weights is not None:\n",
    "            print(\"Encoder: Using pre-trained embeddings.\")\n",
    "            if emb_dim != embedding_weights.shape[1]: print(f\"Warn: Enc emb({emb_dim}) != GloVe({embedding_weights.shape[1]}). Using GloVe.\"); self.actual_emb_dim=embedding_weights.shape[1]\n",
    "            self.embedding=nn.Embedding.from_pretrained(embedding_weights,freeze=freeze_emb,padding_idx=pad_idx)\n",
    "        else: print(\"Encoder: Random embeddings.\"); self.embedding=nn.Embedding(input_dim,self.actual_emb_dim,padding_idx=pad_idx)\n",
    "        print(f\"Encoder emb shape: {self.embedding.weight.shape}, Frozen: {freeze_emb if embedding_weights is not None else 'N/A'}\")\n",
    "        self.gru=nn.GRU(self.actual_emb_dim,hidden_dim,num_layers,dropout=(dropout if num_layers>1 else 0),batch_first=True,bidirectional=True)\n",
    "        self.dropout=nn.Dropout(dropout); self.fc_hidden=nn.Linear(hidden_dim*2,hidden_dim)\n",
    "    def forward(self, src_seq):\n",
    "        embedded = self.dropout(self.embedding(src_seq))\n",
    "        enc_outputs, hidden = self.gru(embedded)\n",
    "        combined_hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        context_vec = torch.tanh(self.fc_hidden(combined_hidden))\n",
    "        processed_hidden = context_vec.unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "        return enc_outputs, processed_hidden\n",
    "\n",
    "class DecoderGRU(nn.Module): # Basic Decoder (No Attention yet)\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, num_layers, dropout, pad_idx=0):\n",
    "        super().__init__(); self.output_dim, self.hidden_dim, self.num_layers = output_dim, hidden_dim, num_layers\n",
    "        print(\"Decoder: Random embeddings.\"); self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=pad_idx)\n",
    "        print(f\"Decoder emb shape: {self.embedding.weight.shape}\")\n",
    "        self.gru=nn.GRU(emb_dim,hidden_dim,num_layers,dropout=(dropout if num_layers > 1 else 0),batch_first=True)\n",
    "        self.fc_out=nn.Linear(hidden_dim,output_dim); self.dropout=nn.Dropout(dropout)\n",
    "    def forward(self, input_step, hidden_state):\n",
    "        input_step=input_step.unsqueeze(1); embedded=self.dropout(self.embedding(input_step))\n",
    "        output,new_hidden=self.gru(embedded,hidden_state); output=output.squeeze(1)\n",
    "        return self.fc_out(output),new_hidden\n",
    "\n",
    "class Seq2SeqGRU(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__(); self.encoder,self.decoder,self.device = encoder,decoder,device\n",
    "        assert encoder.num_layers == decoder.num_layers; assert encoder.hidden_dim == decoder.hidden_dim # After fc_hidden\n",
    "    def forward(self, src_seq, trg_seq, teacher_forcing_ratio=0.5):\n",
    "        batch_size,trg_len,trg_vocab_size = trg_seq.shape[0],trg_seq.shape[1],self.decoder.output_dim\n",
    "        dec_outputs=torch.zeros(batch_size,trg_len,trg_vocab_size).to(self.device)\n",
    "        enc_outputs, dec_hidden_init = self.encoder(src_seq) # enc_outputs sẽ dùng cho Attention\n",
    "        dec_hidden = dec_hidden_init; dec_input = trg_seq[:,0]\n",
    "        for t in range(1,trg_len):\n",
    "            pred, dec_hidden = self.decoder(dec_input, dec_hidden) # Decoder cơ bản chưa dùng enc_outputs\n",
    "            dec_outputs[:,t,:] = pred\n",
    "            if random.random()<teacher_forcing_ratio: dec_input=trg_seq[:,t]\n",
    "            else: dec_input=pred.argmax(1)\n",
    "        return dec_outputs\n",
    "\n",
    "print(\"Model classes (EncoderGRU, DecoderGRU, Seq2SeqGRU) defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d7d582",
   "metadata": {},
   "source": [
    "## Định nghĩa các hàm khởi tạo,huấn luyện, đánh giá, timing, inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25eafb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions (init, train, eval, time, translate) defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Weight Initialization ---\n",
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        is_encoder_embedding = 'encoder.embedding.weight' in name\n",
    "        use_pretrained = 'glove_embeddings' in globals() and glove_embeddings is not None # Check if global exists\n",
    "        is_frozen = FREEZE_GLOVE\n",
    "\n",
    "        if is_encoder_embedding and use_pretrained and is_frozen: continue # Skip frozen\n",
    "        if param.dim() > 1: nn.init.xavier_uniform_(param)\n",
    "        elif 'bias' in name: nn.init.constant_(param, 0)\n",
    "\n",
    "# --- Training & Evaluation Loops ---\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train(); epoch_loss=0; proc_batches=0\n",
    "    print(f\"Starting training epoch...\") # Thêm print này\n",
    "    pbar = tqdm(iterator, desc=f\"Training\", leave=False, dynamic_ncols=True) # Bỏ epoch khỏi desc\n",
    "    for i,batch in enumerate(pbar):\n",
    "        try:\n",
    "            src,trg = batch; src,trg = src.to(DEVICE),trg.to(DEVICE)\n",
    "            optimizer.zero_grad(); output = model(src,trg,TEACHER_FORCING_RATIO)\n",
    "            out_dim = output.shape[-1]; output_loss = output[:,1:].reshape(-1,out_dim); trg_loss = trg[:,1:].reshape(-1)\n",
    "            loss = criterion(output_loss,trg_loss)\n",
    "            if torch.isnan(loss) or torch.isinf(loss): print(f\"NaN/Inf loss at train batch {i}. Skip.\"); continue\n",
    "            loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(),clip); optimizer.step()\n",
    "            epoch_loss+=loss.item(); proc_batches+=1; pbar.set_postfix(loss=f\"{loss.item():.4f}\") # Format loss\n",
    "        except Exception as e: print(f\"\\nTrain batch {i} err: {e}\"); traceback.print_exc(); optimizer.zero_grad(); continue\n",
    "    if proc_batches==0: return float('inf')\n",
    "    return epoch_loss/proc_batches\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval(); epoch_loss=0; proc_batches=0\n",
    "    print(f\"Starting evaluation...\") # Thêm print này\n",
    "    pbar = tqdm(iterator, desc=\"Evaluating\", leave=False, dynamic_ncols=True)\n",
    "    with torch.no_grad():\n",
    "        for i,batch in enumerate(pbar):\n",
    "            try:\n",
    "                src,trg = batch; src,trg = src.to(DEVICE),trg.to(DEVICE)\n",
    "                output = model(src,trg,0) # NO teacher forcing\n",
    "                out_dim=output.shape[-1]; output_loss = output[:,1:].reshape(-1,out_dim); trg_loss = trg[:,1:].reshape(-1)\n",
    "                loss = criterion(output_loss,trg_loss)\n",
    "                if torch.isnan(loss) or torch.isinf(loss): print(f\"NaN/Inf loss at eval batch {i}. Skip.\"); continue\n",
    "                epoch_loss+=loss.item(); proc_batches+=1; pbar.set_postfix(loss=f\"{loss.item():.4f}\") # Format loss\n",
    "            except Exception as e: print(f\"\\nEval batch {i} err: {e}\"); traceback.print_exc(); continue\n",
    "    if proc_batches==0: return float('inf')\n",
    "    return epoch_loss/proc_batches\n",
    "\n",
    "# --- Helper function for timing ---\n",
    "def epoch_time(start_time, end_time):\n",
    "    el_time=end_time-start_time; el_mins=int(el_time/60); el_secs=int(el_time-(el_mins*60)); return el_mins,el_secs\n",
    "\n",
    "# --- Inference Function ---\n",
    "def translate_sentence_hf(sentence, src_tokenizer, trg_tokenizer, model, device, max_len=50):\n",
    "    model.eval()\n",
    "    if not isinstance(sentence,str) or not sentence.strip(): return \"Invalid input.\"\n",
    "    clean_s = normalize_string(sentence); src_enc = src_tokenizer.encode(clean_s)\n",
    "    if not src_enc or not src_enc.ids: return \"Empty after tokenization.\"\n",
    "    src_tensor = torch.LongTensor(src_enc.ids).unsqueeze(0).to(device)\n",
    "    sos_id = trg_tokenizer.token_to_id(SOS_TOKEN)\n",
    "    eos_id = trg_tokenizer.token_to_id(EOS_TOKEN)\n",
    "    if sos_id is None or eos_id is None: print(\"Err: Target SOS/EOS not found.\"); return \"Translation setup err.\"\n",
    "    trg_ids_res = []\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            _, dec_hidden = model.encoder(src_tensor) # Chỉ cần hidden khởi tạo\n",
    "            dec_input = torch.LongTensor([sos_id]).to(device)\n",
    "            for _ in range(max_len):\n",
    "                output, dec_hidden = model.decoder(dec_input, dec_hidden)\n",
    "                pred_id = output.argmax(1).item()\n",
    "                # Không thêm EOS vào kết quả cuối cùng\n",
    "                if pred_id == eos_id: break\n",
    "                trg_ids_res.append(pred_id)\n",
    "                dec_input = torch.LongTensor([pred_id]).to(device)\n",
    "        # Decode bỏ qua special tokens (bao gồm SOS/EOS nếu tokenizer làm vậy)\n",
    "        return trg_tokenizer.decode(trg_ids_res, skip_special_tokens=True)\n",
    "    except Exception as e: print(f\"Inference err: {e}\"); traceback.print_exc(); return \"Translation err.\"\n",
    "\n",
    "print(\"Helper functions (init, train, eval, time, translate) defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a0e91",
   "metadata": {},
   "source": [
    "## Chuẩn bị glove\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "594f6fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Preparing GloVe ---\n",
      "glove.6B.300d.txt exists.\n",
      "GloVe preparation successful or file already exists.\n"
     ]
    }
   ],
   "source": [
    "#Chuẩn bị GloVe\n",
    "print(\"--- Step 1: Preparing GloVe ---\")\n",
    "if not download_file(GLOVE_ZIP_URL, GLOVE_DIR, GLOVE_ZIP_FILENAME):\n",
    "    print(\"GloVe preparation failed. Please ensure the GloVe file exists or download is possible.\")\n",
    "    # Thoát nếu bạn đang chạy script .py\n",
    "    # exit(1)\n",
    "    # Nếu dùng Notebook, bạn có thể dừng ở đây hoặc xử lý khác\n",
    "    raise RuntimeError(\"GloVe preparation failed.\")\n",
    "else:\n",
    "    print(\"GloVe preparation successful or file already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdc0953",
   "metadata": {},
   "source": [
    "## Tải và phân chia dữ liệu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93299f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2 & 3: Loading Raw Data and Splitting ---\n",
      "Successfully read 254090 lines from data/en_sents\n",
      "Successfully read 254090 lines from data/vi_sents\n",
      "Data Split - Train: 203272, Validation: 25409, Test: 25409\n"
     ]
    }
   ],
   "source": [
    "#Tải và Chia Dữ liệu\n",
    "print(\"\\n--- Step 2 & 3: Loading Raw Data and Splitting ---\")\n",
    "en_sents_raw = read_raw_data(EN_FILE)\n",
    "vi_sents_raw = read_raw_data(VI_FILE)\n",
    "\n",
    "if en_sents_raw is None or vi_sents_raw is None or len(en_sents_raw) != len(vi_sents_raw) or not en_sents_raw:\n",
    "    raise RuntimeError(\"Data loading error or mismatch or empty. Exiting.\")\n",
    "\n",
    "combined = list(zip(en_sents_raw, vi_sents_raw)); random.seed(42); random.shuffle(combined)\n",
    "total_len=len(combined); train_len=int(0.8*total_len); valid_len=int(0.1*total_len); test_len=total_len-train_len-valid_len\n",
    "\n",
    "if train_len==0 or valid_len==0 or test_len==0:\n",
    "    raise RuntimeError(f\"Dataset too small ({total_len}) to split. Exiting.\")\n",
    "\n",
    "train_data = combined[:train_len]; valid_data = combined[train_len:train_len+valid_len]; test_data = combined[train_len+valid_len:]\n",
    "en_train_sents, vi_train_sents = zip(*train_data)\n",
    "en_valid_sents, vi_valid_sents = zip(*valid_data)\n",
    "en_test_sents, vi_test_sents = zip(*test_data)\n",
    "\n",
    "print(f\"Data Split - Train: {len(en_train_sents)}, Validation: {len(en_valid_sents)}, Test: {len(en_test_sents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a562234",
   "metadata": {},
   "source": [
    "## Chuẩn bị Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b65f3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 4: Preparing Tokenizers ---\n",
      "Loading tokenizer for en from tokenizers/en_tokenizer.json\n",
      "Enabled padding for en (ID: 0).\n",
      "Loading tokenizer for vi from tokenizers/vi_tokenizer.json\n",
      "Enabled padding for vi (ID: 0).\n",
      "EN Vocab: 20000, VI Vocab: 9660, EN PAD: 0, VI PAD: 0\n"
     ]
    }
   ],
   "source": [
    "#Chuẩn bị Tokenizers\n",
    "print(\"\\n--- Step 4: Preparing Tokenizers ---\")\n",
    "# Pass lists directly as generators were consumed in the previous combined version\n",
    "tokenizer_en = train_or_load_tokenizer(\"en\", list(en_train_sents), vocab_size=MAX_VOCAB_SIZE)\n",
    "tokenizer_vi = train_or_load_tokenizer(\"vi\", list(vi_train_sents), vocab_size=MAX_VOCAB_SIZE)\n",
    "\n",
    "if tokenizer_en is None or tokenizer_vi is None:\n",
    "    raise RuntimeError(\"Tokenizer prep error. Exiting.\")\n",
    "\n",
    "INPUT_VOCAB_SIZE = tokenizer_en.get_vocab_size()\n",
    "OUTPUT_VOCAB_SIZE = tokenizer_vi.get_vocab_size()\n",
    "PAD_IDX_EN = tokenizer_en.token_to_id(PAD_TOKEN)\n",
    "PAD_IDX_VI = tokenizer_vi.token_to_id(PAD_TOKEN)\n",
    "\n",
    "if PAD_IDX_EN is None or PAD_IDX_VI is None:\n",
    "    raise RuntimeError(f\"Error: PAD token missing. Exiting.\")\n",
    "\n",
    "print(f\"EN Vocab: {INPUT_VOCAB_SIZE}, VI Vocab: {OUTPUT_VOCAB_SIZE}, EN PAD: {PAD_IDX_EN}, VI PAD: {PAD_IDX_VI}\")\n",
    "\n",
    "# --- Lưu lại các index cần thiết cho inference ---\n",
    "SOS_IDX_VI = tokenizer_vi.token_to_id(SOS_TOKEN)\n",
    "EOS_IDX_VI = tokenizer_vi.token_to_id(EOS_TOKEN)\n",
    "if SOS_IDX_VI is None or EOS_IDX_VI is None:\n",
    "     print(\"Warning: SOS or EOS index for target language not found in tokenizer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65e054f",
   "metadata": {},
   "source": [
    "## Load Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "875d5c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 5: Loading GloVe Embeddings ---\n",
      "Loading GloVe embeddings from glove_data/glove.6B.300d.txt...\n",
      "Tokenizer vocab size (for embedding matrix): 20000\n",
      "Reading GloVe file (this may take a while)...\n",
      "  Processed 100000 lines...\n",
      "  Processed 200000 lines...\n",
      "  Processed 300000 lines...\n",
      "  Processed 400000 lines...\n",
      "Finished reading 400000 lines.\n",
      "Loaded 14781/20000 words from GloVe file into embedding matrix.\n",
      "Set PAD token embedding (Index: 0) to zeros.\n",
      "Initializing UNK token embedding (Index: 3) randomly.\n",
      "Initializing SOS token embedding (Index: 1) randomly.\n",
      "Initializing EOS token embedding (Index: 2) randomly.\n"
     ]
    }
   ],
   "source": [
    "# Khối 5.4: Load GloVe Embeddings\n",
    "print(\"\\n--- Step 5: Loading GloVe Embeddings ---\")\n",
    "glove_embeddings = load_glove_embeddings(GLOVE_PATH, EMBEDDING_DIM_GLOVE, tokenizer_en)\n",
    "# if glove_embeddings is None:\n",
    "#     print(\"Warning: GloVe failed. Encoder using random embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636e02d6",
   "metadata": {},
   "source": [
    "## Tạo dataset và dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2b11a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 6: Creating Datasets and DataLoaders ---\n",
      "DataLoaders created.\n"
     ]
    }
   ],
   "source": [
    "# Khối 5.5: Tạo Datasets và DataLoaders\n",
    "print(\"\\n--- Step 6: Creating Datasets and DataLoaders ---\")\n",
    "try:\n",
    "    train_dataset = TranslationDatasetHF(en_train_sents, vi_train_sents, tokenizer_en, tokenizer_vi)\n",
    "    valid_dataset = TranslationDatasetHF(en_valid_sents, vi_valid_sents, tokenizer_en, tokenizer_vi)\n",
    "    test_dataset  = TranslationDatasetHF(en_test_sents, vi_test_sents, tokenizer_en, tokenizer_vi)\n",
    "except ValueError as e:\n",
    "     print(f\"Dataset creation error: {e}\"); raise e # Re-raise error\n",
    "\n",
    "collate_with_padding_hf = lambda batch: collate_fn_hf(batch, PAD_IDX_EN, PAD_IDX_VI)\n",
    "\n",
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_with_padding_hf, drop_last=True)\n",
    "valid_iterator = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_with_padding_hf, drop_last=False)\n",
    "test_iterator  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_with_padding_hf, drop_last=False)\n",
    "print(\"DataLoaders created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b075687",
   "metadata": {},
   "source": [
    "## Khởi tạo Model,optimizer và criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7430af97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 7: Initializing Model (with Bidirectional Encoder) ---\n",
      "Encoder: Using pre-trained embeddings.\n",
      "Encoder emb shape: torch.Size([20000, 300]), Frozen: True\n",
      "Decoder: Random embeddings.\n",
      "Decoder emb shape: torch.Size([9660, 256])\n",
      "Model weights initialized.\n",
      "Optimizer & Criterion set.\n"
     ]
    }
   ],
   "source": [
    "#Khởi tạo Model, Optimizer, Criterion\n",
    "print(\"\\n--- Step 7: Initializing Model (with Bidirectional Encoder) ---\")\n",
    "encoder = EncoderGRU(INPUT_VOCAB_SIZE, EMBEDDING_DIM_GLOVE, HIDDEN_DIM, NUM_LAYERS, DROPOUT_RATE,\n",
    "                     embedding_weights=glove_embeddings, freeze_emb=FREEZE_GLOVE, pad_idx=PAD_IDX_EN).to(DEVICE)\n",
    "decoder = DecoderGRU(OUTPUT_VOCAB_SIZE, EMBEDDING_DIM_VI, HIDDEN_DIM, NUM_LAYERS, DROPOUT_RATE,\n",
    "                     pad_idx=PAD_IDX_VI).to(DEVICE)\n",
    "model = Seq2SeqGRU(encoder, decoder, DEVICE).to(DEVICE)\n",
    "\n",
    "# Quan trọng: Áp dụng khởi tạo trọng số SAU KHI chuyển model lên DEVICE\n",
    "model.apply(init_weights); print(\"Model weights initialized.\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX_VI); print(\"Optimizer & Criterion set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0abe414",
   "metadata": {},
   "source": [
    "## Vòng lặp huấn luyện "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0649548c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 8: Starting Training Loop ---\n",
      "\n",
      "Epoch: 01/2\n",
      "Starting training epoch...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464d51606cb241eb9d834970239b8fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Vòng lặp Huấn luyện\n",
    "best_valid_loss = float('inf')\n",
    "print(\"\\n--- Step 8: Starting Training Loop ---\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nEpoch: {epoch+1:02}/{NUM_EPOCHS}\")\n",
    "    # Hàm train và evaluate đã có print bên trong\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    end_time = time.time(); epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    # Lưu model nếu validation loss tốt hơn\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss; torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print(f\"\\t-> Saved Best Model (Val Loss: {valid_loss:.3f})\")\n",
    "    else: print(f\"\\t   Validation loss did not improve from {best_valid_loss:.3f}\")\n",
    "\n",
    "    # In kết quả epoch\n",
    "    print(f'\\tEpoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    if train_loss != float('inf'): print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(min(train_loss, 700)):7.3f}')\n",
    "    else: print('\\tTrain Loss: Inf')\n",
    "    if valid_loss != float('inf'): print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(min(valid_loss, 700)):7.3f}')\n",
    "    else: print('\\t Val. Loss: Inf')\n",
    "print(\"\\n--- Training Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9558907",
   "metadata": {},
   "source": [
    "## Đánh giá trên tập test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40108aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Đánh giá trên Tập Test\n",
    "print(f\"\\n--- Step 9: Evaluating Best Model on Test Set ---\")\n",
    "try:\n",
    "    print(f\"Loading best model from {MODEL_SAVE_PATH} for final test evaluation...\")\n",
    "    # Load vào model instance hiện có (đã có cấu trúc đúng)\n",
    "    model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
    "    print(\"Best model loaded.\")\n",
    "    test_loss = evaluate(model, test_iterator, criterion) # Đánh giá model tốt nhất\n",
    "    if test_loss != float('inf'):\n",
    "        print(f'| Final Test Loss: {test_loss:.3f} | Final Test PPL: {math.exp(min(test_loss, 700)):7.3f} |')\n",
    "    else:\n",
    "        print(\"| Test Loss: Inf |\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Model file '{MODEL_SAVE_PATH}' not found. Skipping test set evaluation.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during test set evaluation: {e}\"); traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187fda7",
   "metadata": {},
   "source": [
    "## Vòng lặp tương tác"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c83bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vòng lặp Tương tác\n",
    "print(f\"\\n--- Step 10 & 11: Loading Best Model & Interactive Translation ---\")\n",
    "interactive_mode = False\n",
    "try:\n",
    "    # Đảm bảo model tốt nhất đã được load ở bước trước hoặc load lại\n",
    "    # model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE)) # Load lại nếu cần\n",
    "    print(\"Using best model loaded previously for interactive mode.\")\n",
    "    # Cần kiểm tra xem tokenizer đã được load thành công chưa\n",
    "    if 'tokenizer_en' in locals() and 'tokenizer_vi' in locals() and tokenizer_en and tokenizer_vi:\n",
    "         interactive_mode = True\n",
    "    else:\n",
    "         print(\"Error: Tokenizers not available for interactive mode.\")\n",
    "\n",
    "except FileNotFoundError: print(f\"Error: Model file '{MODEL_SAVE_PATH}' not found for interactive mode.\")\n",
    "except Exception as e: print(f\"Error loading model for interactive mode: {e}\"); traceback.print_exc()\n",
    "\n",
    "if interactive_mode:\n",
    "    print(\"\\nEnter an English sentence to translate (or type 'quit' to exit):\")\n",
    "    while True:\n",
    "        try:\n",
    "            input_sentence = input(\"> \")\n",
    "            if input_sentence.lower().strip() == 'quit': break\n",
    "            if not input_sentence.strip(): continue\n",
    "            # Gọi hàm translate (đã được định nghĩa ở Khối 4)\n",
    "            translated_sentence = translate_sentence_hf(input_sentence, tokenizer_en, tokenizer_vi, model, DEVICE)\n",
    "            print(f\"VI: {translated_sentence}\")\n",
    "        except KeyboardInterrupt: print(\"\\nExiting interactive mode.\"); break\n",
    "        except Exception as e: print(f\"Translation error: {e}\"); traceback.print_exc()\n",
    "    print(\"\\nExited interactive translation mode.\")\n",
    "else:\n",
    "    print(\"\\nInteractive mode could not be started.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vy_minh_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
